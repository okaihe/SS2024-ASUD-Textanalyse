{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a976870d-13da-48da-9809-2831263e4819",
   "metadata": {},
   "source": [
    "# Datengenerierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587b75b7-eb2a-49f6-b931-d6cf27909aa6",
   "metadata": {},
   "source": [
    "Dieses Notebook stellt die Code bereit, um aus den im Ordner `ntv-data` liegenden NTV-Nachrichtenartikeln eine `data.csv`-Datei zu generieren, die anschließend im Notebook `datenanalyse.ipynb` als Datengrundlage für weitere Analysen verwendet werden kann. Das Einlesen der 5000+ Nachrichtenartikel führt aufgrund der vielen Dateizugriffe zu einer schlechten Performance. Es ist daher sinnvoller, sobald neue Daten hinzukommen, eine einzelne `.csv`-Datei zu generieren, die anschließend mit einem einzelnen Dateizugriff mit bspw. `pandas` eingelesen werden kann."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaa94a1-994d-42fb-974a-08f41f2cf2f4",
   "metadata": {},
   "source": [
    "## Benötige Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815f339-793d-4d3e-8e78-db65aeeef15e",
   "metadata": {},
   "source": [
    "Zunächst müssen die benötigten Python-Packages importiert werden. Zu installieren ist hierfür zunächst das Package `BeautifulSoup` mit folgendem `pip`-Aufruf:\n",
    "\n",
    "```pip install beautifulsoup4```\n",
    "\n",
    "Anschließend können die Packages importiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be5716a2-611e-49be-aff8-8b31509d5e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c543f7-6fe9-4ddb-a7af-2c890c1dcb59",
   "metadata": {},
   "source": [
    "## Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79db9ba-bfba-45d7-91dc-2086216fc4c1",
   "metadata": {},
   "source": [
    "Die Funktion `get_content_of` nimmt einen Dateipfad entgegen und gibt den entsprechenden Inhalt als `string` zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f21e4fd0-0fdf-4154-8246-e9fb739fc0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_of(article_path: str) -> str:\n",
    "    result = \"\"\n",
    "    with open(article_path, \"r\", encoding=\"utf-8\") as article:\n",
    "        result = article.read()\n",
    "    if result != \"\":\n",
    "        return result\n",
    "    raise Exception(\"File not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd89c9a4-0589-48cc-8602-55c03b72c4c6",
   "metadata": {},
   "source": [
    "Mit Hilfe der `extract_text_from_html_article`-Funktion extrahieren wir den eigentlichen Nachrichtentext aus der `html`-Datei und geben diesen als `string` zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae69f63f-9081-44e9-a7ce-e8948c01ca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_html_article(html_text: str) -> str:\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    article_text = soup.find(\"div\", class_=\"article__text\")\n",
    "    if article_text:\n",
    "        soup = BeautifulSoup(article_text.text, \"html.parser\")\n",
    "        for aside in soup.find_all(\"div\", class_=\"article__aside\"):\n",
    "            aside.decompose()\n",
    "        output = soup.text.strip()\n",
    "        output = re.sub(\" +\", \" \", output)\n",
    "        output = output.replace(\"\\n\", \"\")\n",
    "        return output\n",
    "    raise Exception(\"No article text was found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c4dbd30-9354-4a02-9a57-877a9d11ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_exact_date_from_html_article(html_text: str) -> str:\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    date = soup.find(\"span\", class_=\"article__date\")\n",
    "    if date:\n",
    "        output = date.get_text()\n",
    "        return output\n",
    "    print(\"No date was found\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57171ee2-ee61-4769-be22-e4bef7652877",
   "metadata": {},
   "source": [
    "Die Überschrift des jeweiligen Artikels kann aus dem Dateinamen abgeleitet werden. Die Funktion `convert_filename_to_article_headline` erfüllt genau diese Funktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a3e10f1-fdad-4c15-9449-6e504474f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_filename_to_article_headline(article_filename: str) -> str:\n",
    "    headline = (\" \").join(article_filename.split(\"-\")[:-1])\n",
    "    return headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0024199-27cc-4b33-ad9a-41cf86593637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ntv_datetime(date_string: str):\n",
    "    try:\n",
    "        datetime_obj = datetime.strptime(date_string, \"%d.%m.%Y, %H:%M Uhr\")\n",
    "        date = datetime_obj.date()\n",
    "        time = datetime_obj.time()\n",
    "        return date, time\n",
    "    except:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10949e3e-15f9-4483-8d67-142b7f6671e2",
   "metadata": {},
   "source": [
    "\n",
    "Die letzte Funktion, `collect_article_contents_from`, sammelt nun, unter Verwendung der obigen Funktionen, die Aritkel und generiert eine Liste, in der die jeweiligen Einträge jeweils die Informationen und Inhalte eines Artikels darstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26137d7a-1b42-4779-b15b-c53ddc26fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_article_contents_from(path: str) -> List[Dict[str, str]]:\n",
    "    collection = []\n",
    "    dates = os.listdir(path)\n",
    "    count_articles = 0\n",
    "    print()\n",
    "    for date in dates:\n",
    "        categories = os.listdir(f\"{path}/{date}\")\n",
    "        for category in categories:\n",
    "            article_list = os.listdir(f\"{path}/{date}/{category}\")\n",
    "            for article in article_list:\n",
    "                if article != \"rss.json\" and article != \".ipynb_checkpoints\":\n",
    "                    count_articles += 1\n",
    "                    print('\\r', end='')\n",
    "                    print(f\"Scanned articles: {count_articles} - scanning: {article}\", end='')\n",
    "                    article_content = get_content_of(f\"{path}/{date}/{category}/{article}\")\n",
    "                    article_text = extract_text_from_html_article(article_content)\n",
    "                    article_extracted_date = extract_exact_date_from_html_article(article_content)\n",
    "                    article_date, article_time = parse_ntv_datetime(article_extracted_date)\n",
    "                    collection.append(\n",
    "                        {\n",
    "                            \"date\": article_date,\n",
    "                            \"time\": article_time,\n",
    "                            \"category\": category,\n",
    "                            \"headline\": convert_filename_to_article_headline(article),\n",
    "                            \"filename\": article,\n",
    "                            \"text\": article_text,\n",
    "                        }\n",
    "                    )\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6246fbb6-97e3-477d-83af-4c3207afcd77",
   "metadata": {},
   "source": [
    "## Datengenerierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bcc7f5-b9aa-489b-8dae-71db6045784b",
   "metadata": {},
   "source": [
    "Damit kann nun der Inhalt der Artikel gesammelt werden und anschließend mit Hilfe des Packages `csv` die `1-1-data.csv`-Datei mit Nachrichtenartikeln generiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1d1c6b4-e1bb-4ad3-8e83-4f1e89c63e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scanned articles: 1022 - scanning: Ulm-im-Pokal-gegen-die-Bayern-VfB-in-Muenster-gefordert-article24982731.htmlhtmllhtmlrticle24976528.html8.htmlml4993852.htmltml1.htmlhtmlticle24999004.html"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1m/1kc7bh856v32jwgwrwc239g00000gn/T/ipykernel_62852/2097671092.py:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(article_text.text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanned articles: 10683 - scanning: Zoelle-gegen-China-Autos-wuerden-uns-aermer-machen-article25003580.htmlmlhtmlhtmll-Schlaganfall-und-Herzinfarkt-article25005778.htmln-Frankenstein-Einheiten-helfen-article25015701.html"
     ]
    }
   ],
   "source": [
    "content = collect_article_contents_from(\"ntv-data\")\n",
    "with open(\"2-1-data.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as output_file:\n",
    "    fc = csv.DictWriter(output_file, fieldnames=content[0].keys())\n",
    "    fc.writeheader()\n",
    "    fc.writerows(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
