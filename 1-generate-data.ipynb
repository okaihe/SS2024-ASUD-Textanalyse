{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a976870d-13da-48da-9809-2831263e4819",
   "metadata": {},
   "source": [
    "# Datengenerierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587b75b7-eb2a-49f6-b931-d6cf27909aa6",
   "metadata": {},
   "source": [
    "Dieses Notebook stellt die Code bereit, um aus den im Ordner `ntv-data` liegenden NTV-Nachrichtenartikeln eine `data.csv`-Datei zu generieren, die anschließend im Notebook `datenanalyse.ipynb` als Datengrundlage für weitere Analysen verwendet werden kann. Das Einlesen der 5000+ Nachrichtenartikel führt aufgrund der vielen Dateizugriffe zu einer schlechten Performance. Es ist daher sinnvoller, sobald neue Daten hinzukommen, eine einzelne `.csv`-Datei zu generieren, die anschließend mit einem einzelnen Dateizugriff mit bspw. `pandas` eingelesen werden kann."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaa94a1-994d-42fb-974a-08f41f2cf2f4",
   "metadata": {},
   "source": [
    "## Benötige Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815f339-793d-4d3e-8e78-db65aeeef15e",
   "metadata": {},
   "source": [
    "Zunächst müssen die benötigten Python-Packages importiert werden. Zu installieren ist hierfür zunächst das Package `BeautifulSoup` mit folgendem `pip`-Aufruf:\n",
    "\n",
    "```pip install beautifulsoup4```\n",
    "\n",
    "Anschließend können die Packages importiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5716a2-611e-49be-aff8-8b31509d5e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from typing import Dict, List\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c543f7-6fe9-4ddb-a7af-2c890c1dcb59",
   "metadata": {},
   "source": [
    "## Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79db9ba-bfba-45d7-91dc-2086216fc4c1",
   "metadata": {},
   "source": [
    "Die Funktion `get_content_of` nimmt einen Dateipfad entgegen und gibt den entsprechenden Inhalt als `string` zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e4fd0-0fdf-4154-8246-e9fb739fc0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_of(article_path: str) -> str:\n",
    "    result = \"\"\n",
    "    with open(article_path, \"r\", encoding=\"utf-8\") as article:\n",
    "        result = article.read()\n",
    "    if result != \"\":\n",
    "        return result\n",
    "    raise Exception(\"File not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd89c9a4-0589-48cc-8602-55c03b72c4c6",
   "metadata": {},
   "source": [
    "Mit Hilfe der `extract_text_from_html_article`-Funktion extrahieren wir den eigentlichen Nachrichtentext aus der `html`-Datei und geben diesen als `string` zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae69f63f-9081-44e9-a7ce-e8948c01ca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_html_article(html_text: str) -> str:\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    article_text = soup.find(\"div\", class_=\"article__text\")\n",
    "    if article_text:\n",
    "        soup = BeautifulSoup(article_text.text, \"html.parser\")\n",
    "        for aside in soup.find_all(\"div\", class_=\"article__aside\"):\n",
    "            aside.decompose()\n",
    "        output = soup.text.strip()\n",
    "        output = re.sub(\" +\", \" \", output)\n",
    "        output = output.replace(\"\\n\", \"\")\n",
    "        return output\n",
    "    raise Exception(\"No article text was found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57171ee2-ee61-4769-be22-e4bef7652877",
   "metadata": {},
   "source": [
    "Die Überschrift des jeweiligen Artikels kann aus dem Dateinamen abgeleitet werden. Die Funktion `convert_filename_to_article_headline` erfüllt genau diese Funktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e10f1-fdad-4c15-9449-6e504474f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_filename_to_article_headline(article_filename: str) -> str:\n",
    "    headline = (\" \").join(article_filename.split(\"-\")[:-1])\n",
    "    return headline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10949e3e-15f9-4483-8d67-142b7f6671e2",
   "metadata": {},
   "source": [
    "Die letzte Funktion, `collect_article_contents_from`, sammelt nun, unter Verwendung der obigen Funktionen, die Aritkel und generiert eine Liste, in der die jeweiligen Einträge jeweils die Informationen und Inhalte eines Artikels darstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26137d7a-1b42-4779-b15b-c53ddc26fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_article_contents_from(path: str) -> List[Dict[str, str]]:\n",
    "    collection = []\n",
    "    dates = os.listdir(path)\n",
    "    count_articles = 0\n",
    "    print()\n",
    "    for date in dates:\n",
    "        categories = os.listdir(f\"{path}/{date}\")\n",
    "        for category in categories:\n",
    "            article_list = os.listdir(f\"{path}/{date}/{category}\")\n",
    "            for article in article_list:\n",
    "                if article != \"rss.json\":\n",
    "                    count_articles += 1\n",
    "                    print('\\r', end='')\n",
    "                    print(f\"Scanned articles: {count_articles} - scanning: {article}\", end='')\n",
    "                    article_content = get_content_of(f\"{path}/{date}/{category}/{article}\")\n",
    "                    article_text = extract_text_from_html_article(article_content)\n",
    "                    collection.append(\n",
    "                        {\n",
    "                            \"date\": date,\n",
    "                            \"category\": category,\n",
    "                            \"headline\": convert_filename_to_article_headline(article),\n",
    "                            \"filename\": article,\n",
    "                            \"text\": article_text,\n",
    "                        }\n",
    "                    )\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6246fbb6-97e3-477d-83af-4c3207afcd77",
   "metadata": {},
   "source": [
    "## Datengenerierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bcc7f5-b9aa-489b-8dae-71db6045784b",
   "metadata": {},
   "source": [
    "Damit kann nun der Inhalt der Artikel gesammelt werden und anschließend mit Hilfe des Packages `csv` die `data.csv`-Datei mit Nachrichtenartikeln generiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d1c6b4-e1bb-4ad3-8e83-4f1e89c63e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = collect_article_contents_from(\"ntv-data\")\n",
    "with open(\"data.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as output_file:\n",
    "    fc = csv.DictWriter(output_file, fieldnames=content[0].keys())\n",
    "    fc.writeheader()\n",
    "    fc.writerows(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
